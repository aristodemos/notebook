% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file.

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf MemC3: Compact and Concurrent MemCache with Dumber Caching and Smarter Hashing}

%for single author (just remove % characters)
\author{
{\rm Aris\ Paphitis}\\
CUT
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Key Point}
The authors present a set of algorithmic and engineering improvements of the popular Memcached system, improving its memory efficiency and throughput.

\section{Background}
Memcached is a popular in-memory caching layer enabled by the demand of low-latency access to data, required by many Internet services. This requirement many system designers to serve data (all or most) from main memory, using the memory either as primary store or as a cache to deflect particular data structures.
The most important metrics are performance (measured as throughput: queries pre second) and memory efficiency (measured by the overhead required to store an item).
Memcached implements a simple and lightweight key-value interface where all key-value tuples are stored in and served from DRAM. Clients communicate with the Memcached servers over the network using a set of commands.
Standard Memcached uses a typical hash table design, with link-list-based chaining to handle collisions. Chaining is efficient for inserting or deleting single keys, however lookup may require scanning the entire chain.
Its cache replace algorithm is strict LRU, also based on linked lists. The least recently used entry is evicted and replaced by a newly inserted entry when the cache is full. The design relies on locking to ensure consistency among multiple threads, leading to poor scaling on multi-core CPUs.
Memory allocation is slab-based. Memory is divided to 1MB pages, and each page is sub-divided into fixed-length chunks. The size of a chunk, and the number of chunks per page, depends on the slab class. Key-value objects are stored in an appropriately-sized chunk. To insert a new key, Memcached looks up the slab class whose chunk size best fits this object. If no vacant chunk is available the search fails and cache eviction is executed.
Memcached was originally single threaded. It uses libevent for asynchronous network I/O callbacks. Later versions support multi-threading via global locks, preventing Memcached from scaling on multi-core CPUs.

\section{Observations}
The authors, studying a recent publication by Facebook on several key-value store workloads, made the following observations.
First, queries for small objects dominate. Most keys are smaller than 32 bytes (Memcached max limit is 250 bytes) and most values no more than a few hundred bytes. The consequence of storing such small keys is high memory overhead. Memcached always allocates a 56-byte header (on 64-bit servers) for each key-value object regardless of size. Thus a more efficient data structure for the index cache should be employed.
Second, queries are read heavy. The studied workloads are characterized by 30x more reads than writes. Some applications have even more skewed workloads with 500x more writes. Though most queries are GETs, this operation is not optimized and locks are still used extensively. A more optimistic approach can benefit by removing all mutexes from the GET path.

\section{Proposed Enhancements}

The authors present a compact, concurrent and cache-aware hashing scheme, which they call optimistic concurrent cuckoo hashing. MemC3 manages to achieve high memory efficiency and increased throughput than original memcached by adapting to a more specific type of workload. This is accomplished by a adopting a more optimistic hashing scheme, a CLOCK-based eviction algorithm and an optimistic locking strategy.

\subsection{Optimistic Concurrent Cuckoo Caching}
The basic idea of cuckoo hashing is the use of two hashing functions instead of one, thus providing each key two possible locations where it can reside. MemC3's hash table is 4-way set-associative by providing an array of buckets, each having 4 slots. This design enables higher (almost double) space utilization. 
Additionally, to support keys of variable length, each slot stores a pointer to the key-value and a short hash of the key called a tag. This results in a more cache friendly  lookup by avoiding long chains of pointer dereferences. The 1-Byte tag is compared in each lookup and the full key is retrieved upon a match. Tag collisions are below 1\% and pointer dereferences are almost eliminated.
Tags are also used to avoid retrieving full keys during insert operations, originally needed to derive the alternate location to displace keys. The hashing scheme used allows insert operations to operate using only information in the table without the need to ever retrieve keys.
To enable concurrency deadlock risks and false misses have to be taken into consideration.
Previous studies suggest breaking up inserts into atomic displacement with the effect of adding space overhead much higher than basic cuckoo hashing. MemC3 allows multiple concurrent reads and single writer through optimistic locks using version counters.
To eliminate false misses MemC3 changes the order of insertion by first searching for a valid cuckoo path and then moving the keys backward along the cuckoo path. As a result, each swap affects only a key at a time, which can be successfully moved to its new location without any kick-out. Inserts are farther improved by employing multiple cuckoo paths. 

\subsection{Concurrent Cache Management}
Basic memcached maintains a doubly linked list to implement its cache eviction strategy which is strictly LRU. This becomes a major source of space overhead, requiring 18 Bytes for each key. This is scheme is also a synchronization bottleneck since all updates to the cache must be serialized.
MemC3 uses a CLOCK-based approach which approximates LRU with improved concurrency and space efficiency. For each slab class a circular buffer and a virtual hand is maintained. Each bit in the buffer represents the recency of a different key-value object. Each update sets this bit to 1. Each evict checks the bit currently pointed by the hand. If it is 0, evict selects the corresponding key-value object; else the bit is reset to 0 and the hand is advanced in the circular buffer until a bit of 0 is encountered. 
In MemC3, the version counters that are  used for optimistic locking are also used to  coordinate evicts with reads to ensure safety.

\section{Evaluation}
Firstly the cuckoo hashing technique, employed by MemC3, is evaluated against the basic chaining scheme of memcached with respect to space efficiency and construction speed. It is shown that MemC3 consumes 5 Bytes less than basic chaining per key, leading to higher memory efficiency. On the other hand the construction rate is faster in chaining hashing, since only the head is modified, but the cost of long chains (up to 13 objects in a chain vs 4 cuckoo hashing) is also present.
Next, the insertion latency with respect to the table remaining space is demonstrated to increase gracefully. Following up, it is shown that using 2 cuckoo paths reduces the insertion latency but using more than that has diminishing returns. 
MemC3 scales up to 6 concurrent threads with 90\% reads and up to 16 threads without any writes, unlike the default hash table which does not scale for either workload since all hash operations are serialized. However, when more write traffic is generated performance  of cuckoo hash tables declines.
MemC3's cache throughput also scales with increasing number of threads and 100\% cache hits. When the size of memory is limited and cache hits are limited to 85\% poor scaling is observed. 
Space efficiency is improved due to the elimination of unneeded pointers in MemC3. Consequently cache misses are reduced.
MemC3 manages to achieve 3x higher throughput and 30\% more space efficiency by applying 
optimizations for read-mostly workloads of key-value objects small in size.



{\footnotesize \bibliographystyle{acm}
}


\theendnotes

\end{document}